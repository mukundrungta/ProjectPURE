


































































100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68124/68124 [02:12<00:00, 512.85it/s]
Number of meta-train features = 8064
Number of meta-test features = 8064
Number of normal features = 68124




















100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22631/22631 [00:41<00:00, 551.33it/s]












100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15509/15509 [00:27<00:00, 556.88it/s]








 96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎         | 10423/10844 [00:16<00:00, 636.46it/s]
Total steps: 21285
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10844/10844 [00:16<00:00, 649.52it/s]






  3%|███████▊                                                                                                                                                                                                                                                       | 130/4257 [00:14<07:43,  8.90it/s]meta_learning/train_tacred.py:100: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)





































































































































 59%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                         | 2499/4257 [04:41<02:54, 10.06it/s]
{'dev_f1': 55.05277458459609}
{'test_f1': 54.477479081360144}
{'challenge_test_f1': 33.17084769944121}
ACCURACY:   58.47%
POSITIVE ACCURACY:   28.93%
NEGATIVE ACCURACY:   82.12%
-------------------------------------------------------------------
TRUE POSITIVE:   0.129 		 (NUMBER:   1394)
FALSE POSITIVE:   0.099 		 (NUMBER:   1077)
TRUE NEGATIVE:   0.456 		 (NUMBER:   4948)
FALSE NEGATIVE:   0.316 		 (NUMBER:   3425)
-------------------------------------------------------------------


































































































100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4257/4257 [08:49<00:00,  8.04it/s]
0it [00:00, ?it/s]
Traceback (most recent call last):
  File "meta_learning/train_tacred.py", line 298, in <module>
    main()
  File "meta_learning/train_tacred.py", line 294, in main
    train_meta_learning(args, model, train_features_meta_train, train_features_meta_test, train_features, benchmarks)
  File "meta_learning/train_tacred.py", line 136, in train_meta_learning
    diffopt.step(meta_train_loss) # computing temporary params on meta-train set
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/higher/optim.py", line 233, in step
    allow_unused=True  # boo
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/autograd/__init__.py", line 228, in grad
    inputs, allow_unused, accumulate_grad=False)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/autograd/function.py", line 87, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore[attr-defined]
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/utils/checkpoint.py", line 97, in backward
    "Checkpointing is not compatible with .grad() or when an `inputs` parameter"
RuntimeError: Checkpointing is not compatible with .grad() or when an `inputs` parameter is passed to .backward(). Please use .backward() and do not pass its `inputs` argument.
Traceback (most recent call last):
  File "meta_learning/train_tacred.py", line 298, in <module>
    main()
  File "meta_learning/train_tacred.py", line 294, in main
    train_meta_learning(args, model, train_features_meta_train, train_features_meta_test, train_features, benchmarks)
  File "meta_learning/train_tacred.py", line 136, in train_meta_learning
    diffopt.step(meta_train_loss) # computing temporary params on meta-train set
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/higher/optim.py", line 233, in step
    allow_unused=True  # boo
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/autograd/__init__.py", line 228, in grad
    inputs, allow_unused, accumulate_grad=False)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/autograd/function.py", line 87, in apply
    return self._forward_cls.backward(self, *args)  # type: ignore[attr-defined]
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/utils/checkpoint.py", line 97, in backward
    "Checkpointing is not compatible with .grad() or when an `inputs` parameter"
RuntimeError: Checkpointing is not compatible with .grad() or when an `inputs` parameter is passed to .backward(). Please use .backward() and do not pass its `inputs` argument.