

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:02<00:00, 407.71it/s]
Traceback (most recent call last):
  File "train_retacred.py", line 254, in <module>
    main()
  File "train_retacred.py", line 235, in main
    train_features_meta_train, train_features_meta_test = processor.read_meta_learning(train_file)
  File "/nethome/mrungta8/project/ProjectPURE/Mukund/RE_improved_baseline/prepro.py", line 224, in read_meta_learning
    input_ids, new_ss, new_os = self.tokenize(tokens, d['subj_type'], d['obj_type'], ss, se, os, oe)
  File "/nethome/mrungta8/project/ProjectPURE/Mukund/RE_improved_baseline/prepro.py", line 67, in tokenize
    tokens_wordpiece = self.tokenizer.tokenize(token)
  File "/usr/local/lib/python3.6/dist-packages/transformers-2.3.0-py3.6.egg/transformers/tokenization_utils.py", line 646, in tokenize
    def lowercase_text(t):
KeyboardInterrupt
Traceback (most recent call last):
  File "train_retacred.py", line 254, in <module>
    main()
  File "train_retacred.py", line 235, in main
    train_features_meta_train, train_features_meta_test = processor.read_meta_learning(train_file)
  File "/nethome/mrungta8/project/ProjectPURE/Mukund/RE_improved_baseline/prepro.py", line 224, in read_meta_learning
    input_ids, new_ss, new_os = self.tokenize(tokens, d['subj_type'], d['obj_type'], ss, se, os, oe)
  File "/nethome/mrungta8/project/ProjectPURE/Mukund/RE_improved_baseline/prepro.py", line 67, in tokenize
    tokens_wordpiece = self.tokenizer.tokenize(token)
  File "/usr/local/lib/python3.6/dist-packages/transformers-2.3.0-py3.6.egg/transformers/tokenization_utils.py", line 646, in tokenize
    def lowercase_text(t):
KeyboardInterrupt