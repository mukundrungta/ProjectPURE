




































































100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58465/58465 [02:19<00:00, 420.41it/s]
Number of train features = 500
Number of test features = 500





















100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19584/19584 [00:44<00:00, 443.08it/s]













100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13418/13418 [00:29<00:00, 448.81it/s]










100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10844/10844 [00:20<00:00, 534.11it/s]
Total steps: 4565
Warmup steps: 456
  0%|                                                                                                                                           | 0/913 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers-2.3.0-py3.6.egg/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
  0%|▍                                                                                                                                  | 3/913 [00:01<05:21,  2.83it/s]
Traceback (most recent call last):
  File "train_retacred.py", line 252, in <module>
    main()
  File "train_retacred.py", line 248, in main
    train(args, model, train_features, benchmarks)
  File "train_retacred.py", line 42, in train
    outputs = model(**inputs)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/autocast_mode.py", line 198, in decorate_autocast
    return func(*args, **kwargs)
  File "/nethome/mrungta8/project/ProjectPURE/Mukund/RE_improved_baseline/model.py", line 25, in forward
    attention_mask=attention_mask,
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/transformers-2.3.0-py3.6.egg/transformers/modeling_bert.py", line 740, in forward
    encoder_attention_mask=encoder_extended_attention_mask)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/transformers-2.3.0-py3.6.egg/transformers/modeling_bert.py", line 386, in forward
    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/transformers-2.3.0-py3.6.egg/transformers/modeling_bert.py", line 366, in forward
    intermediate_output = self.intermediate(attention_output)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/transformers-2.3.0-py3.6.egg/transformers/modeling_bert.py", line 328, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/usr/local/lib/python3.6/dist-packages/transformers-2.3.0-py3.6.egg/transformers/modeling_bert.py", line 133, in gelu
    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
RuntimeError: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 10.76 GiB total capacity; 9.15 GiB already allocated; 81.44 MiB free; 9.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_retacred.py", line 252, in <module>
    main()
  File "train_retacred.py", line 248, in main
    train(args, model, train_features, benchmarks)
  File "train_retacred.py", line 42, in train
    outputs = model(**inputs)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/autocast_mode.py", line 198, in decorate_autocast
    return func(*args, **kwargs)
  File "/nethome/mrungta8/project/ProjectPURE/Mukund/RE_improved_baseline/model.py", line 25, in forward
    attention_mask=attention_mask,
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/transformers-2.3.0-py3.6.egg/transformers/modeling_bert.py", line 740, in forward
    encoder_attention_mask=encoder_extended_attention_mask)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/transformers-2.3.0-py3.6.egg/transformers/modeling_bert.py", line 386, in forward
    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/transformers-2.3.0-py3.6.egg/transformers/modeling_bert.py", line 366, in forward
    intermediate_output = self.intermediate(attention_output)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/transformers-2.3.0-py3.6.egg/transformers/modeling_bert.py", line 328, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "/usr/local/lib/python3.6/dist-packages/transformers-2.3.0-py3.6.egg/transformers/modeling_bert.py", line 133, in gelu
    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
RuntimeError: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 10.76 GiB total capacity; 9.15 GiB already allocated; 81.44 MiB free; 9.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF