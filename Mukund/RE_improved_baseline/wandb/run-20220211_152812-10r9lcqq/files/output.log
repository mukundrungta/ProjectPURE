



100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:06<00:00, 778.57it/s]
  9%|███████████▍                                                                                                                   | 451/5000 [00:00<00:05, 893.69it/s]
Number of train features = 58465


100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:05<00:00, 902.15it/s]


100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:05<00:00, 922.36it/s]
0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers-2.3.0-py3.6.egg/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
1it [00:01,  1.12s/it]
Total steps: 73080
Warmup steps: 7308
Traceback (most recent call last):
  File "train_retacred.py", line 248, in <module>
    main()
  File "train_retacred.py", line 245, in main
    train_meta_learning(args, model, train_features_meta_train, train_features_meta_test, benchmarks)
  File "train_retacred.py", line 98, in train_meta_learning
    with torch.backends.cudnn.flags(enabled=False), higher.innerloop_ctx(model, inner_opt, copy_initial_weights=False) as (fast_model, diffopt):
  File "/usr/lib/python3.6/contextlib.py", line 81, in __enter__
    return next(self.gen)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/higher/__init__.py", line 97, in innerloop_ctx
    track_higher_grads=track_higher_grads
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/higher/optim.py", line 779, in get_diff_optim
    **kwargs
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/higher/optim.py", line 101, in __init__
    self.param_groups = _copy.deepcopy(other.param_groups)
  File "/usr/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.6/copy.py", line 215, in _deepcopy_list
    append(deepcopy(a, memo))
  File "/usr/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.6/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.6/copy.py", line 215, in _deepcopy_list
    append(deepcopy(a, memo))
  File "/usr/lib/python3.6/copy.py", line 161, in deepcopy
    y = copier(memo)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/nn/parameter.py", line 32, in __deepcopy__
    result = type(self)(self.data.clone(memory_format=torch.preserve_format), self.requires_grad)
RuntimeError: CUDA out of memory. Tried to allocate 198.00 MiB (GPU 0; 10.76 GiB total capacity; 9.31 GiB already allocated; 41.44 MiB free; 9.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_retacred.py", line 248, in <module>
    main()
  File "train_retacred.py", line 245, in main
    train_meta_learning(args, model, train_features_meta_train, train_features_meta_test, benchmarks)
  File "train_retacred.py", line 98, in train_meta_learning
    with torch.backends.cudnn.flags(enabled=False), higher.innerloop_ctx(model, inner_opt, copy_initial_weights=False) as (fast_model, diffopt):
  File "/usr/lib/python3.6/contextlib.py", line 81, in __enter__
    return next(self.gen)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/higher/__init__.py", line 97, in innerloop_ctx
    track_higher_grads=track_higher_grads
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/higher/optim.py", line 779, in get_diff_optim
    **kwargs
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/higher/optim.py", line 101, in __init__
    self.param_groups = _copy.deepcopy(other.param_groups)
  File "/usr/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.6/copy.py", line 215, in _deepcopy_list
    append(deepcopy(a, memo))
  File "/usr/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.6/copy.py", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.6/copy.py", line 150, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.6/copy.py", line 215, in _deepcopy_list
    append(deepcopy(a, memo))
  File "/usr/lib/python3.6/copy.py", line 161, in deepcopy
    y = copier(memo)
  File "/nethome/mrungta8/.local/lib/python3.6/site-packages/torch/nn/parameter.py", line 32, in __deepcopy__
    result = type(self)(self.data.clone(memory_format=torch.preserve_format), self.requires_grad)
RuntimeError: CUDA out of memory. Tried to allocate 198.00 MiB (GPU 0; 10.76 GiB total capacity; 9.31 GiB already allocated; 41.44 MiB free; 9.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF